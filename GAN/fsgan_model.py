import torch
from torch import nn
from torch.autograd import Variable


class Encoder(nn.Module):
    def __init__(self):
        super(Encoder, self).__init__()
        # x = 10 x 1 x 3 x 400  [batch_size, channels, height, width]
        self.encoder = nn.Sequential(
            # in_ch, out_ch, kernel_size
            nn.Conv2d(1, 8, 3, stride=2, padding=1),  # x = 10 x 8 x 2 x 200
            nn.ReLU(True),
            nn.MaxPool2d(2, stride=1),  # x = 10 x 8 x 1 x 199
            nn.Conv2d(8, 1, 3, stride=1, padding=1),  # x = 10 x 1 x 1 x 199
            nn.ReLU(True),
        )
        self.pool = nn.MaxPool1d(2, stride=1),

    def forward(self, x):
        x = self.encoder(x)
        x = x.view(-1, 1, x.shape[-1])  # x = 10 x 1 x 199
        return self.pool(x)  # x = 10 x 1 x 198


class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.height = 3  # height of the generated tensor
        self.width = 400  # width of the generated tensor
        self.in_ch = 1  # number of input channels
        self.latent_dim = 198  # length of the latent representation (generated by encoder per batch)

        self.genNet = nn.Sequential(
            nn.ConvTranspose1d(in_channels=self.in_ch,
                               out_channels=self.in_ch * 32,
                               kernel_size=3,
                               stride=1, padding=1, bias=False),  # x = 10 x 32 x 198
            nn.BatchNorm1d(num_features=self.in_ch * 32),
            nn.ReLU(inplace=True),

            nn.ConvTranspose1d(in_channels=self.in_ch * 32,
                               out_channels=self.in_ch * 16,
                               kernel_size=3,
                               stride=1, padding=1, bias=False),  # x = 10 x 16 x 198
            nn.BatchNorm1d(num_features=self.in_ch * 16),
            nn.ReLU(inplace=True),

            nn.ConvTranspose1d(in_channels=self.in_ch * 16,
                               out_channels=self.in_ch * 8,
                               kernel_size=3,
                               stride=1, padding=0, bias=False),
            # x = 10 x 8 x 200
            nn.BatchNorm1d(num_features=self.in_ch * 8),
            nn.ReLU(inplace=True),

            nn.ConvTranspose1d(in_channels=self.in_ch * 8,
                               out_channels=self.height,
                               kernel_size=4, stride=2, padding=1, bias=False),
            # x = 10 x 3 x 400
            nn.Tanh()
            )

    def forward(self, latent_vector):
        x = self.genNet(latent_vector)
        x = x.view(x.shape[0],
                   self.in_ch,
                   self.height,
                   self.width)
        return x  # x = 10 x 1 x 3 x 400


class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.height = 3
        self.width = 400
        self.num_channels = 1
        self.ndf = 64

        self.discNet = nn.Sequential(
            # x = 1 x 1 x 3 x 400
            nn.Conv2d(in_channels=self.num_channels,
                      out_channels=self.ndf,
                      kernel_size=3, stride=2, padding=1, bias=False),
            nn.LeakyReLU(negative_slope=0.2, inplace=True),

            # x = 1 x 64 x 2 x 200
            nn.Conv2d(in_channels=self.ndf,
                      out_channels=self.ndf * 2,
                      kernel_size=2, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(num_features=self.ndf * 2),
            nn.LeakyReLU(negative_slope=0.2, inplace=True),

            # x = 1 x 128 x 2 x 101
            nn.Conv2d(in_channels=self.ndf * 2,
                      out_channels=1, kernel_size=2, stride=1, padding=0,
                      bias=False),
            nn.Sigmoid())

    def forward(self, x):
        x = self.discNet(x)  # x = 1 x 1 x 3 x 102
        return x.view(-1, 1).squeeze(1)  # x = torch.Size[306]


